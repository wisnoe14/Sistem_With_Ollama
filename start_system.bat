@echo off
echo ===========================================
echo ðŸš€ ICONNET CS Simulation System Launcher
echo ===========================================
echo.

REM Check if Ollama is running
echo â³ Checking Ollama service...
curl -s http://localhost:11434/api/tags > nul 2>&1
if %errorlevel% neq 0 (
    echo âŒ Ollama not running! Please start Ollama first:
    echo    ollama serve
    echo.
    pause
    exit /b 1
) else (
    echo âœ… Ollama service is running
)

REM Check if backend is running
echo â³ Checking backend service...
curl -s http://localhost:8000/ > nul 2>&1
if %errorlevel% neq 0 (
    echo âŒ Backend not running! Starting backend...
    echo.
    echo ðŸ”§ Starting FastAPI Backend...
    cd backend
    start cmd /k "uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload"
    cd ..
    echo âœ… Backend started in new window
    timeout /t 3 > nul
) else (
    echo âœ… Backend service is running
)

REM Check if frontend is running
echo â³ Checking frontend service...
curl -s http://localhost:5173/ > nul 2>&1
if %errorlevel% neq 0 (
    echo âŒ Frontend not running! Starting frontend...
    echo.
    echo ðŸŽ¨ Starting React Frontend...
    cd frontend
    start cmd /k "npm run dev"
    cd ..
    echo âœ… Frontend started in new window
    timeout /t 3 > nul
) else (
    echo âœ… Frontend service is running
)

echo.
echo ===========================================
echo ðŸŽ‰ System Status:
echo âœ… Ollama AI: http://localhost:11434
echo âœ… Backend API: http://localhost:8000
echo âœ… Frontend UI: http://localhost:5173
echo ===========================================
echo.
echo ðŸŒ Opening application in browser...
timeout /t 2 > nul
start http://localhost:5173

echo.
echo ðŸ“ System Ready! You can now:
echo   1. Test dynamic AI conversation generation
echo   2. Use Telecollection, Winback, or Retention flows
echo   3. Experience context-aware question generation
echo.
echo Press any key to exit launcher...
pause > nul